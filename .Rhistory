source('~/Coursera/StormData/.Rprofile')
qunif(0.75,0,1)
N = 9
mean = 1100
sd = 30
error = qt(0.975, df = N-1) * sd / sqrt(N)
ans = mean + c(-1,1) * error
round(ans)
ans
N = 9
mean = -2
# error = qt(0.95,df = 9)*sd/sqrt(N-1) = 2
ans = 2 * sqrt(N) / qt(0.975, df = N-1)
round(ans, 2)
nx = 10
ny = 10
mx = 3
my = 5
Sx2 = 0.6
Sy2 = 0.68
Sr = sqrt(((nx-1)*Sx2+(ny-1)*Sy2)/(nx+ny-2))
ans = mx - my + c(-1,1) * qt(0.975, df = nx + ny - 2) * Sr * sqrt(1/nx+1/ny)
round(ans, 2)
nx = 100
ny = 100
mx = 4
my = 6
Sx2 = 0.5
Sy2 = 2
Sr = sqrt(((nx-1)*Sx2+(ny-1)*Sy2)/(nx+ny-2))
ans = my - mx + c(-1,1) * qt(0.975, df = nx + ny - 2) * Sr * sqrt(1/nx+1/ny)
round(ans, 2)
nD = 9
nP = 9
mD = -3
mP = 1
sD = 1.5
sP = 1.8
Sr = sqrt(((nD-1)*sD^2+(nP-1)*sP^2)/(nD+nD-2))
ans = mD - mP + c(-1,1) * qt(0.9, df = nD + nP - 2) * Sr * sqrt(1/nD+1/nP)
ans
nx = 100
ny = 100
mx = 4
my = 6
Sx2 = 0.5
Sy2 = 2
Sr = sqrt(((nx-1)*Sx2+(ny-1)*Sy2)/(nx+ny-2))
ans = my - mx + c(-1,1) * qt(0.975, df = nx + ny - 2) * Sr * sqrt(1/nx+1/ny)
round(ans, 2)
nD = 9
nP = 9
mD = -3
mP = 1
sD = 1.5
sP = 1.8
Sr = sqrt(((nD-1)*sD^2+(nP-1)*sP^2)/(nD+nD-2))
ans = mD - mP + c(-1,1) * qt(0.9, df = nD + nP - 2) * Sr * sqrt(1/nD+1/nP)
ans
a <- c(140, 138, 150, 148, 135)
b <- c(132, 135, 151, 146, 130)
t.test(a, b, alternative = "two.sided", paired = T)
# Manual method
mu <- sum(w*x)/sum(w)
round(mu,4)
# Alternate method:
lm(x ~ 1, weights = w)$coefficients
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
# Manual method
mu <- sum(w*x)/sum(w)
round(mu,4)
# Alternate method:
lm(x ~ 1, weights = w)$coefficients
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ 0 + x)$coefficients
data(mtcars)
lm(mpg ~ wt, data = mtcars)$coefficients
$$ \begin{align} \hat \beta_1 &= Cor(X,Y) \frac{Sd(Y)}{Sd(X)} \ &= (0.5) \frac{Sd(Y)}{0.5Sd(Y)} \ &= 1 \end{align} $$
Sd(Y) = 1
B1=0.51/0.5
B1
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
xc<-(x-mean(x))/sd(x)
xc
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
coef(lm(y~x))
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
install.packages("swirl")
library(swirl)
swirl()
install_from_swirl("Regression Models")
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent,galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
fit <- lm(child ~ parent, galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs-rhs
all.equal(lhs, rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild,varEst+varRes)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
cor(gpa_nor,gch_nor)
l_nor <- lm(gch_nor ~ gpa_nor)
fit <- lm(child ~ parent, galton)
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(fit)
1-sRes/sTot
summary(fit)$r.squared
cor(galton$parent,galton$child)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent -1, galton)
lm(child ~ parent, galton)
lm(child ~ 1, galton)
view(trees)
head(trees)
fit <- lm(Volume ~ Girth + Height + Constant -1, trees)
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2 <- lm(Volume ~ Height + Constant -1, trees2)
lapply(list(fit, fit2), coef)
all <- lm(Fertility ~ ., swiss)
summary(all)
summary(lm(Fertility ~ Agriculture, swiss))
cor(swiss$Agriculture,swiss$Education)
cor(swiss$Examination,swiss$Education)
cor(swiss$Agriculture,swiss$Education)
makelms()
ec <- swiss$Examination+swiss$Catholic
efit <- lm(Fertility ~ . + ec, swiss)
all$coefficients - efit$coefficients
6
dim(InsectSprays)
head(InsectSprays,15)
E
sE
summary(InsectSprays[,2])
sapply(InsectSprays,class)
fit <- lm(count ~ spray, InsectSprays)
summary(fit)$coef
est <- summary(fit)$coef[,1]
mean(sA)
mean(sB)
nfit <- lm(count ~ spray - 1, InsectSprays)
summary(nfit)$coef
relevel(InsectSprays$spray,C)
spray2 <- relevel(InsectSprays$spray,"C")
fit2 <- lm(count ~ spray2, InsectSprays)
summary(fit2)$coef
mean(sC)
(fit$coef[2]-fit$coef[3])/1.6011
dim(hunger)
948
names(hunger)
fit <- lm(hunger$Numeric ~ hunger$Year)
summary(fit$coef)
summary(fit)$coef
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])
lmF <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])
lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex)
summary(lmBoth)
lmInter <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Year * hunger$Sex)
summary(lmInter)
fit <- lm(y ~ x, out2)
plot(fit, which=1)
fitno <- lm(y ~ x, out2[-1, ])
plot(fitno, which=1)
coef(fit)-coef(fitno)
head(dfbeta(fit))
resno <- out2[1, "y"] - predict(fitno, out2[1,])
1-resid(fit)[1]/resno
head(hatvalues(fit))
sigma <- sqrt(deviance(fit)/df.residual(fit))
rstd <- resid(fit)/(sigma * sqrt(1-hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which=3)
plot(fit, which=2)
resid(fit)[1]/(sigma1*sqrt(1-hatvalues(fit)[1]))
sigma1 <- sqrt(deviance(fitno)/df.residual(fitno))
resid(fit)[1]/(sigma1*sqrt(1-hatvalues(fit)[1]))
head(rstudent(fit))
dy <- predict(fitno, out2)-predict(fit, out2)
sum(dy^2)/(2*sigma^2)
plot(fit, which=5)
rgp1()
rgp2()
head(swiss)
mdl <- lm(Fertility ~ ., swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ . -Examination, swiss)
vif(mdl2)
apply(x1c, 1, mean)
x1c <- simbias()
apply(x1c, 1, mean)
fit1 <- lm(Fertiliy ~ Agriculture, swiss)
fit1 <- lm(Fertility ~ Agriculture, swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
anova(fit1, fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- deviance(fit1)-deviance(fit3)
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3,fit5, fit6)
ravenData
mdl <- glm(ravenWinNum ~ ravenScore, binomial, ravenData)
lodds <- predict(mdl, data.frame(ravenScore=c(0, 3, 6))
;
lodds <- predict(mdl, data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(0.95, 1)
var(rpois(1000, 50))
nxt()
head(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
confint(mdl, 'date')
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
mdl2 <- offset=log(visits+1)
mdl2 <- glm(simplystats ~ date, poisson, hits, offset=log(visits+1))
qpois(.95, mdl2$fitted.values[704])
?mtcars
install.packages(c("car", "digest", "googleVis", "httr", "jsonlite", "RColorBrewer", "RCurl", "reshape2"))
install.packages(c("car", "digest", "googleVis", "httr", "jsonlite",
"RColorBrewer", "RCurl", "reshape2"))
install.packages(c("car", "digest", "googleVis", "httr", "jsonlite",
"RColorBrewer", "RCurl", "reshape2"))
install.packages(c("car", "digest", "googleVis", "httr", "jsonlite",
"RColorBrewer", "RCurl", "reshape2"))
library(ggplot2)
library(knitr)
library("ggplot2", lib.loc="~/R/win-library/3.1")
install.packages("ggplot2")
install.packages("ggplot2")
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
# To compare model we usually use an anova table
# anova null hypothesis says that both models are the same.
compare <- anova(fit1, fit2)
compare$Pr
install.packages("caret")
install.packages(c("digest", "kernlab", "manipulate", "rmarkdown", "rstudioapi", "swirl"))
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
rm(list = ls())
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
model_rf = train(diagnosis ~ ., method = 'rf', data = training)
model_gbm = train(diagnosis ~ ., method = 'gbm', data = training)
model_lda = train(diagnosis ~ ., method = 'lda', data = training)
pred_rf = predict(model_rf, training)
pred_gbm = predict(model_gbm, training)
pred_lda = predict(model_lda, training)
comb_data = data.frame(rf = pred_rf, gbm = pred_gbm, lda = pred_lda, diagnosis = training$diagnosis)
model_comb = train(diagnosis ~ ., method = 'rf', data = comb_data)
pred_rf_test = predict(model_rf, testing)
pred_gbm_test = predict(model_gbm, testing)
pred_lda_test = predict(model_lda, testing)
comb_data_test = data.frame(rf = pred_rf_test, gbm = pred_gbm_test, lda = pred_lda_test, diagnosis = testing$diagnosis)
pred_comb_test = predict(model_comb, comb_data_test)
accuracy_rf = sum(pred_rf_test == testing$diagnosis) / length(pred_rf_test)
accuracy_gbm = sum(pred_gbm_test == testing$diagnosis) / length(pred_gbm_test)
accuracy_lda = sum(pred_lda_test == testing$diagnosis) / length(pred_lda_test)
accuracy_comb = sum(pred_comb_test == comb_data_test$diagnosis) / length(pred_comb_test)
install.packages("AppliedPredictiveModeling")
rm(list = ls())
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
model_rf = train(diagnosis ~ ., method = 'rf', data = training)
install.packages(c("manipulate", "rmarkdown"))
rm(list = ls())
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
model_rf = train(diagnosis ~ ., method = 'rf', data = training)
model_gbm = train(diagnosis ~ ., method = 'gbm', data = training)
model_lda = train(diagnosis ~ ., method = 'lda', data = training)
pred_rf = predict(model_rf, training)
pred_gbm = predict(model_gbm, training)
pred_lda = predict(model_lda, training)
comb_data = data.frame(rf = pred_rf, gbm = pred_gbm, lda = pred_lda, diagnosis = training$diagnosis)
model_comb = train(diagnosis ~ ., method = 'rf', data = comb_data)
pred_rf_test = predict(model_rf, testing)
pred_gbm_test = predict(model_gbm, testing)
pred_lda_test = predict(model_lda, testing)
comb_data_test = data.frame(rf = pred_rf_test, gbm = pred_gbm_test, lda = pred_lda_test, diagnosis = testing$diagnosis)
pred_comb_test = predict(model_comb, comb_data_test)
accuracy_rf = sum(pred_rf_test == testing$diagnosis) / length(pred_rf_test)
accuracy_gbm = sum(pred_gbm_test == testing$diagnosis) / length(pred_gbm_test)
accuracy_lda = sum(pred_lda_test == testing$diagnosis) / length(pred_lda_test)
accuracy_comb = sum(pred_comb_test == comb_data_test$diagnosis) / length(pred_comb_test)
accuracy_comb
accuracy_rf
accuracy_gbm
accuracy_lda
rm(list = ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
model = train(CompressiveStrength ~ ., method = 'lasso', data = training)
rm(list = ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
model = train(CompressiveStrength ~ ., method = 'lasso', data = training)
plot(model$finalModel)
install.packages("bats")
rm(list = ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
model = svm(CompressiveStrength ~ ., data = training)
model
pred = predict(model, testing)
RMSE = sqrt(sum((pred - testing$CompressiveStrength)^2))
predins = predict(model, training)
RMSEins = sqrt(sum((predins - training$CompressiveStrength)^2))
RMSEins = sqrt(sum((predins - training$CompressiveStrength)^2))
library(e1071)
set.seed(325)
model = svm(CompressiveStrength ~ ., data = training)
model
pred = predict(model, testing)
RMSE = sqrt(sum((pred - testing$CompressiveStrength)^2))
predins = predict(model, training)
RMSEins = sqrt(sum((predins - training$CompressiveStrength)^2))
RMSEins
RMSE
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
myPlot
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step=0.1))
getwd
getwd()
dir()
dir()
setwd("C:/Users/Joseph Taylor/Documents")
dir()
setwd("C:/Users/Joseph Taylor/Documents/Coursera")
dir()
setwd("C:/Users/Joseph Taylor/Documents/Coursera/Developing Data Products")
getwed()
getwd()
dir()
dir()
)
data$hp <= y, ]
tmp <- data[data$cyl == x & data$hp <= y, ]
library(shiny)
install.packages("shiny")
data(mtcars)
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
"#FF7F00"))
shinyServer(function(input, output, session) {
))
runApp()
install.packages(c("BradleyTerry2", "car", "KernSmooth", "manipulate"))
install.packages("shinyFiles")
install.packages("shiny")
library(Shiny)
library(shiny)
runApp()
runApp()
runApp()
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='jrt23',
token='80DE054C548F98C6CEB15538F187E488',
secret='vf5WTJoTMqpxcazjpVVx8EWMEnBIaMnlIFGIu7rV')
getwd()
library(shinyapps)
shinyapps::deployApp('C:/Users/Joseph Taylor/Documents/Coursera/Developing Data Products')
library(shinyapps)
deployApp()
dir()
deployApp('mtcars')
library(shinyapps)
shinyapps::deployApp('')
dir()
deployApp('C:\Users\Joseph Taylor\Documents\Coursera\Developing Data Products')
deployApp('C:\\Users\\Joseph Taylor\\Documents\\Coursera\\Developing Data Products')
setwd(Error: /v1/applications/ 400 - Validation error: field 'name' is invalid application name. application name must be at least 4 characters, and may only contain letters, numbers, dashes and hyphens..)
setwd(C:/Users/Joseph Taylor/Documents/Coursera/Developing Data Products)
setwd("C:/Users/Joseph Taylor/Documents/Coursera/Developing Data Products"")
""
setwd("C:/Users/Joseph Taylor/Documents/Coursera/Developing Data Products")
deployApp()
